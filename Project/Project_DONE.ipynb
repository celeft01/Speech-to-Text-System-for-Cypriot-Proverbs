{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee3028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import librosa\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import evaluate\n",
    "import Levenshtein\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0566b4-af6c-4cdf-a1e7-9bc73667ebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "test_path = r\"C:\\PATH\\test\"\n",
    "train_path = r\"C:\\PATH\\train\"\n",
    "txt_path = r\"C:\\PATH\\Paroimies.txt\"\n",
    "\n",
    "# Load the text data\n",
    "#Reads the file at txt_path line by line.\n",
    "#Extracts proverbs using a regular expression (re.match), (e.g., 1. First proverb).\n",
    "def load_target_text(txt_path):\n",
    "    target_texts = []\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                match = re.match(r\"^\\d+\\.\\s*(.*)\", line)\n",
    "                if match:\n",
    "                    text = match.group(1).strip()\n",
    "                    target_texts.append(text)\n",
    "                else:\n",
    "                    print(f\"Skipping line due to formatting issues: {line}\")\n",
    "    return target_texts\n",
    "\n",
    "target_texts = load_target_text(txt_path) # saves the returned list of proverbs\n",
    "\n",
    "# Preprocess audio and text data\n",
    "#Resamples the audio to 16 kHz (sr=16000), which is required by Whisper for consistent feature extraction.\n",
    "def preprocess_audio(file_path, target_text):\n",
    "    # Load audio and resample to 16 kHz (required for Whisper)\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    return {\"audio\": audio, \"text\": target_text} #returns the resampled audio with the corresponding targeted text\n",
    "\n",
    "def create_dataset(data_path, target_texts):\n",
    "    data = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            match = re.match(r\"(\\d+)\", filename)#Uses a regular expression to extract a numeric ID from the filename (e.g., 1.wav would extract 1).\n",
    "            #This ID is used to match the audio file to its corresponding proverb in target_texts.\n",
    "            if match:\n",
    "                file_id = int(match.group(1))\n",
    "                file_path = os.path.join(data_path, filename)\n",
    "                target_sentence = target_texts[file_id - 1] if 0 < file_id <= len(target_texts) else \"\"\n",
    "                data.append(preprocess_audio(file_path, target_sentence))\n",
    "    return data\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_data = create_dataset(train_path, target_texts)\n",
    "test_data = create_dataset(test_path, target_texts)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12889762-b466-43a9-95cd-33b66001c7fa",
   "metadata": {},
   "source": [
    "Model Weights: Contains pre-trained parameters for Whisper-small.\n",
    "Processor Configuration: Includes details about expected input features (e.g., spectrogram size, sampling rate) and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ebd3d98-1294-484a-9692-b8df02691cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model and processor\n",
    "# Downloads and loads the pre-trained Whisper processor for the small model.\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\") \n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514ef51-e05e-48cd-b8f2-68bc526c95c4",
   "metadata": {},
   "source": [
    "Prepares a single data point (audio + text) into a format suitable for training or inference with Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b31161-de3f-42ee-9263-f916ce085293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e2d8db2114bc7b5bbff83810dfa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086580c1165a40d8a0047c7b670365b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess dataset for Whisper\n",
    "def preprocess_batch(batch):\n",
    "    # Convert audio to Whisper input features\n",
    "    input_features = processor(batch[\"audio\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    # Tokenize the text labels\n",
    "    #Converts the text into a sequence of token IDs (integers corresponding to tokens in the Whisper vocabulary).\n",
    "    labels = processor.tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"longest\").input_ids\n",
    "    # Store tensors directly\n",
    "    batch[\"input_features\"] = input_features.squeeze(0)  # Ensure shape [feature_dim, time]\n",
    "    batch[\"labels\"] = labels.squeeze(0)  # Ensure shape [sequence_length]\n",
    "    return batch\n",
    "\n",
    "# Map preprocessing to train and test datasets\n",
    "# Applies the preprocess_batch to all data points\n",
    "train_dataset = train_dataset.map(preprocess_batch)\n",
    "test_dataset = test_dataset.map(preprocess_batch)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"audio\", \"text\"])\n",
    "test_dataset = test_dataset.remove_columns([\"audio\", \"text\"])\n",
    "#The training and testing datasets will only contain:\n",
    "# input_features: The processed audio data.\n",
    "# labels: The processed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdb6e0-7b4c-4e18-ab39-3475c0c38fd2",
   "metadata": {},
   "source": [
    "Batching Input Features:\n",
    "\n",
    "Stacks mel-spectrograms (input_features) into a single tensor for efficient parallel processing.\n",
    "Padding Labels:\n",
    "\n",
    "Ensures all text sequences (labels) in a batch are padded to the same length, avoiding dimension mismatch during training.\n",
    "Compatibility with Whisper:\n",
    "\n",
    "Whisper models require inputs and labels to be provided as batched tensors for training or inference.\n",
    "The data collator ensures that both input_features and labels meet these requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114bfe4d-2084-4479-b152-b200c30fec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Data Collator\n",
    "class DataCollatorWhisper:\n",
    "    def __call__(self, features):\n",
    "        input_features = torch.stack([torch.tensor(feature[\"input_features\"]) for feature in features])\n",
    "        labels = [torch.tensor(feature[\"labels\"]) for feature in features]\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "data_collator = DataCollatorWhisper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e8764e0-638f-4359-a7f1-97e62ce9f804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/01 01:53:17 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id c059f6b7fa3a4ff1abfbd4b26ce8e4cf: Failed to log run data: Exception: Changing param values is not allowed. Param with key='num_train_epochs' was already logged with value='5' for run ID='c059f6b7fa3a4ff1abfbd4b26ce8e4cf'. Attempted logging new value '3'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/231 14:29:47, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.192499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.035852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 1:08:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 0.01320820115506649, 'eval_wer': 0.035852178709321565, 'eval_runtime': 2119.8971, 'eval_samples_per_second': 0.124, 'eval_steps_per_second': 0.016, 'epoch': 2.9806451612903224}\n"
     ]
    }
   ],
   "source": [
    "# Load WER metric\n",
    "# WER = Sunstitution + deletions+insertions / Num of words\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Transliteration map (partial, extend as needed)\n",
    "greek_to_greeklish = {\n",
    "    \"α\": \"a\", \"β\": \"v\", \"γ\": \"g\", \"δ\": \"d\", \"ε\": \"e\", \"ζ\": \"z\",\n",
    "    \"η\": \"i\", \"θ\": \"th\", \"ι\": \"i\", \"κ\": \"k\", \"λ\": \"l\", \"μ\": \"m\",\n",
    "    \"ν\": \"n\", \"ξ\": \"x\", \"ο\": \"o\", \"π\": \"p\", \"ρ\": \"r\", \"σ\": \"s\",\n",
    "    \"τ\": \"t\", \"υ\": \"y\", \"φ\": \"f\", \"χ\": \"ch\", \"ψ\": \"ps\", \"ω\": \"o\",\n",
    "    \"ς\": \"s\"\n",
    "}\n",
    "\n",
    "def transliterate_to_greeklish(greek_text):\n",
    "    return ''.join(greek_to_greeklish.get(char, char) for char in greek_text)\n",
    "\n",
    "# Compute metrics\n",
    "#Decodes predictions and labels into text format.\n",
    "#Transliterates predictions to Greeklish.\n",
    "#Computes WER between the predicted and reference texts\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_texts_greeklish = [transliterate_to_greeklish(text) for text in pred_texts]\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_texts_greeklish, references=label_texts)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# Custom Trainer to force language settings during generation\n",
    "#Extracts input features and labels from the batch.\n",
    "#Applies generation (model.generate) with Greek language settings (\"el\").\n",
    "#Computes loss when labels are available.\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        ignore_keys = ignore_keys or []\n",
    "        inputs = {k: v.to(self.args.device) for k, v in inputs.items() if k not in ignore_keys}\n",
    "        has_labels = \"labels\" in inputs\n",
    "        labels = inputs[\"labels\"] if has_labels else None\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[\"loss\"].mean().detach()\n",
    "            return (loss, None, None)\n",
    "\n",
    "        generation_inputs = inputs[\"input_features\"]\n",
    "        gen_kwargs = {\"language\": \"el\", \"task\": \"transcribe\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(generation_inputs, **gen_kwargs)\n",
    "\n",
    "        loss = None\n",
    "        if has_labels:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[\"loss\"].mean().detach()\n",
    "\n",
    "        return (loss, generated_tokens, labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# Initialize Custom Trainer\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a432fd7d-1ba6-4c49-9c68-02d6ee5b087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "\n",
    "model.save_pretrained(\"./whisper-finetuned\")\n",
    "processor.save_pretrained(\"./whisper-finetuned\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53cb8ddf-9b01-4a82-a4eb-ea99a6d5dfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.01320820115506649,\n",
       " 'eval_wer': 0.035852178709321565,\n",
       " 'eval_runtime': 2067.991,\n",
       " 'eval_samples_per_second': 0.127,\n",
       " 'eval_steps_per_second': 0.016,\n",
       " 'epoch': 2.9806451612903224}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7bfb83-38f5-43e8-8b28-6b0deee2348c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to your saved model directory\n",
    "model_path = r\"C:\\PATH\\whisper-finetuned\"\n",
    "\n",
    "# Load the model and processor\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9fb0bd-8cbf-42a0-bacd-0cc89076f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Apou polla logia o kosmos en dikos tou.\n"
     ]
    }
   ],
   "source": [
    "# Test on unseen audio\n",
    "def transcribe_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    generated_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# Example transcription\n",
    "test_audio_path = r\"C:\\PATH\\audio.wav\"\n",
    "print(\"Transcription:\", transcribe_audio(test_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c629114-aa6e-49c3-81b2-71d1eed20c36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in c:\\users\\foivos\\anaconda3\\lib\\site-packages (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\foivos\\anaconda3\\lib\\site-packages (from Levenshtein) (3.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install  Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ba5ec4-37e5-4823-84b0-fa480bae0c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the nearest proverb using Levenshtein distance\n",
    "def find_nearest_proverb(transcription, target_texts):\n",
    "    \"\"\"Find the closest match for the transcription from the list of proverbs.\"\"\"\n",
    "    distances = {proverb: Levenshtein.distance(transcription.lower(), proverb.lower()) for proverb in target_texts}\n",
    "    nearest_proverb = min(distances, key=distances.get)  # Proverb with the smallest distance\n",
    "    distance = distances[nearest_proverb]\n",
    "    return nearest_proverb, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cd1408-269b-487d-9f96-a11c6b42da3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_and_suggest(file_path, target_texts):\n",
    "    # Step 1: Transcribe the audio\n",
    "    transcription = transcribe_audio(file_path)\n",
    "\n",
    "    # Step 2: Find the nearest proverb\n",
    "    nearest_proverb, distance = find_nearest_proverb(transcription, target_texts)\n",
    "\n",
    "    # Step 3: Return transcription and the closest proverb\n",
    "    return transcription, nearest_proverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e16098-9f2f-4d02-85ae-63483fcb7406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Apou polla logia o kosmos en dikos tou.\n",
      "Do you mean: Apou’n antrepetai, o kosmos en dikos tou.\n"
     ]
    }
   ],
   "source": [
    "test_audio_path = r\"C:\\PATH\\audio.wav\"\n",
    "transcription, closest_proverb = transcribe_and_suggest(test_audio_path, target_texts)\n",
    "print(\"Transcription:\", transcription)\n",
    "print(\"Do you mean:\", closest_proverb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367774c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf4a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c055a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8863cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8e464a-9799-4a01-9f33-ad4b0e214179",
   "metadata": {},
   "outputs": [
    
    
   ],
   "source": [
    "# Load a CSV file\n",
    "testing_df = pd.read_csv(r'C:\\Users\\Foivos\\Desktop\\DS\\Semester3\\DSC515\\projec\\ALISAVOU\\train_test_df.csv')\n",
    "\n",
    "rows_n = testing_df.shape[0]\n",
    "transcription_counter = 0\n",
    "closest_counter = 0\n",
    "\n",
    "# Using all the initial data, find out what percentage of the audio recording's transcriptions are right on the 1st try, and on the 2nd try.\n",
    "for i in range(rows_n):\n",
    "    transcription, closest_proverb = transcribe_and_suggest(testing_df.iloc[i, 0], target_texts)\n",
    "    if transcription==testing_df.iloc[i, 1]:\n",
    "        transcription_counter+=1\n",
    "        closest_counter+=1\n",
    "    elif closest_proverb==testing_df.iloc[i, 1]:\n",
    "        closest_counter+=1\n",
    "\n",
    "transcription_percent = (transcription_counter/rows_n)*100\n",
    "closest_percent = (closest_counter/rows_n)*100\n",
    "\n",
    "print(\"Transcription percentage: \", transcription_percent)\n",
    "print(\"Closest percentage: \", closest_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843f15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_env)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
